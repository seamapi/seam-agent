"""
Demo script to run actual investigations with the enhanced SimpleInvestigator.

This shows the dynamic tool selection and limit enforcement in real action.
"""

import asyncio
import os
from unittest.mock import AsyncMock, Mock, patch

from src.seam_agent.assistant.simple_investigator import SimpleInvestigator
from src.seam_agent.assistant.investigation_config import InvestigationConfig


async def demo_investigation_with_mocks():
    """Run a realistic investigation with mocked external services."""

    print("üîç RUNNING ENHANCED SEAM AGENT INVESTIGATION")
    print("=" * 60)

    # Mock all external dependencies
    with (
        # patch(
        #     "src.seam_agent.assistant.simple_investigator.DatabaseClient"
        # ) as mock_db_class,
        # patch(
        #     "src.seam_agent.assistant.simple_investigator.SeamAPIClient"
        # ) as mock_seam_class,
        patch(
            "src.seam_agent.assistant.simple_investigator.AsyncAnthropic"
        ) as mock_anthropic_class,
        patch(
            "src.seam_agent.assistant.simple_investigator.SupportQueryParser"
        ) as mock_parser_class,
    ):
        print("üìã Setting up mocked services...")

        # Configure Anthropic mock
        mock_anthropic = AsyncMock()
        mock_anthropic_class.return_value = mock_anthropic

        # Configure query parser mock
        mock_parser = Mock()
        mock_parser_class.return_value = mock_parser

        # Configure tool orchestrator mock
        mock_tool_orchestrator = Mock()
        mock_tool_orchestrator.get_tool_definitions.return_value = [
            {"name": "get_device_info", "description": "Get device info"},
            {"name": "get_access_codes", "description": "Get access codes"},
            {"name": "get_audit_logs", "description": "Get audit logs"},
            {"name": "get_admin_links", "description": "Get admin links"},
        ]

        # Create investigator with test config
        config = InvestigationConfig(
            MAX_TOOL_ROUNDS=3, MAX_TOOLS_PER_ROUND=2, MAX_TOTAL_TOOLS=5
        )

        investigator = SimpleInvestigator(
            api_key="demo-key", debug_mode=True, config=config
        )

        print(
            f"‚úÖ Investigator created with limits: {config.MAX_TOOL_ROUNDS} rounds, {config.MAX_TOTAL_TOOLS} total tools"
        )
        print()

        # Test Query - Access Code Issue
        test_query = """
        Jake
        Jul 21st at 4:27 AM
        Hi team,
        Can you help me check if this unmanaged code on this users lock is something that we created but only marked by the system as unmanaged?
        Lock name: Cabin Basement
        Lock ID: 14923119-8686-4010-870f-0621cdef220a
        Code name: Brian Kem5f2
        Code: 7995
        Access Code: aeae87ee-5f52-498f-8208-e2daf28bcfa0
        Looking at the timing of when this unmanaged code was created, it lines up with when we first attempted to create the code but got rejected.
        Perhaps it was actually created/added on the lock but then somehow got marked as unmanaged?
        """

        print("üéØ CUSTOMER QUERY:")
        print("-" * 40)
        print(test_query.strip())
        print()

        # Mock parsed query (what our query parser would extract)
        mock_parsed_query = Mock()
        mock_parsed_query.question_type = "access_code"
        mock_parsed_query.device_ids = ["14923119-8686-4010-870f-0621cdef220a"]
        mock_parsed_query.access_codes = ["aeae87ee-5f52-498f-8208-e2daf28bcfa0"]
        mock_parsed_query.confidence = 0.95
        mock_parsed_query.__dict__ = {
            "question_type": "access_code",
            "device_ids": ["14923119-8686-4010-870f-0621cdef220a"],
            "access_codes": ["aeae87ee-5f52-498f-8208-e2daf28bcfa0"],
            "confidence": 0.95,
        }

        mock_parser.parse.return_value = mock_parsed_query

        print("üß† QUERY ANALYSIS:")
        print("-" * 40)
        print(f"Issue Type: {mock_parsed_query.question_type}")
        print(f"Device ID: {mock_parsed_query.device_ids[0]}")
        print(f"Access Code: {mock_parsed_query.access_codes[0]}")
        print(f"Confidence: {mock_parsed_query.confidence}")
        print()

        # Check what tools dynamic selector recommends
        initial_tools = investigator.dynamic_tool_selector.select_initial_tools(
            mock_parsed_query, test_query
        )
        print("üõ†Ô∏è  DYNAMIC TOOL SELECTION:")
        print("-" * 40)
        print(f"Recommended initial tools: {initial_tools}")
        print(
            "Reasoning: Access code issue detected ‚Üí device info + access codes + audit logs"
        )
        print()

        # Mock the investigation flow
        print("üîÑ INVESTIGATION FLOW:")
        print("-" * 40)

        # Round 1: Initial tool call
        print("Round 1: Initial Investigation")
        mock_tool_response = Mock()
        mock_tool_response.content = [
            Mock(
                type="tool_use",
                id="tool_1",
                name="get_device_info",
                input={"device_id": "14923119-8686-4010-870f-0621cdef220a"},
            ),
            Mock(
                type="tool_use",
                id="tool_2",
                name="get_access_codes",
                input={
                    "device_id": "14923119-8686-4010-870f-0621cdef220a",
                    "workspace_id": "test",
                },
            ),
        ]

        # Mock tool execution results
        investigator.tool_orchestrator.execute_tool = AsyncMock()
        investigator.tool_orchestrator.execute_tool.side_effect = [
            # Device info result
            {
                "device_type": "nuki_lock",
                "display_name": "Cabin Basement",
                "properties": {"online": True},
                "workspace_id": "customer-workspace",
            },
            # Access codes result
            {
                "access_codes": [
                    {
                        "name": "Brian Kem5f2",
                        "code": "7995",
                        "is_managed": False,
                        "access_code_id": "aeae87ee-5f52-498f-8208-e2daf28bcfa0",
                    },
                    {"name": "Main Code", "code": "1234", "is_managed": True},
                ],
                "pagination": {"has_more": False},
            },
        ]

        investigator.tool_orchestrator.summarize_tool_result = Mock()
        investigator.tool_orchestrator.summarize_tool_result.side_effect = [
            "Device Info: nuki_lock in workspace customer-workspace (online)",
            "Access Codes (2 total): Brian Kem5f2: 7995, Main Code: 1234 - 1 unmanaged code found",
        ]

        # Round 2: Follow-up based on findings
        mock_followup_response = Mock()
        mock_followup_response.content = [
            Mock(
                type="tool_use",
                id="tool_3",
                name="get_audit_logs",
                input={"device_id": "14923119-8686-4010-870f-0621cdef220a"},
            )
        ]

        # Mock audit logs result
        investigator.tool_orchestrator.execute_tool.side_effect.append(
            {
                "audit_logs": [
                    {
                        "action": "INSERT",
                        "table": "access_codes",
                        "timestamp": "2025-07-21T04:25:00Z",
                        "old_values": None,
                        "new_values": {"name": "Brian Kem5f2", "is_managed": True},
                    },
                    {
                        "action": "UPDATE",
                        "table": "access_codes",
                        "timestamp": "2025-07-21T04:27:00Z",
                        "old_values": {"is_managed": True},
                        "new_values": {"is_managed": False},
                    },
                ]
            }
        )

        investigator.tool_orchestrator.summarize_tool_result.side_effect.append(
            "Audit Logs: 2 entries found (access code changes, creations, deletions)"
        )

        # Final analysis
        mock_final_response = Mock()
        mock_final_response.content = [
            Mock(
                type="text",
                text="""Based on my investigation, I found clear evidence that explains this unmanaged code issue:

**Device Analysis:**
- Device: Nuki P√≤sit 1 (Cabin Basement) - Status: Online
- Device Type: nuki_lock
- Workspace: customer-workspace

**Access Code Analysis:**
- Found the specific code: "Brian Kem5f2" (7995)
- Current Status: Unmanaged (is_managed: False)
- Access Code ID matches: aeae87ee-5f52-498f-8208-e2daf28bcfa0

**Timeline Investigation:**
- 04:25:00 - Code initially created as MANAGED (is_managed: True)
- 04:27:00 - Code status changed to UNMANAGED (is_managed: False)

**Root Cause:**
This confirms Jake's suspicion! The code WAS created successfully by your system as a managed code at 04:25:00. However, 2 minutes later at 04:27:00, the system marked it as unmanaged, likely due to a synchronization issue with the Nuki API.

**Recommendation:**
1. This appears to be a race condition in the Nuki integration
2. The code is valid and functional - it just needs to be re-marked as managed
3. Consider implementing a delay or retry mechanism for Nuki API status checks""",
            )
        ]

        # Set up the Anthropic call sequence
        mock_anthropic.messages.create.side_effect = [
            mock_tool_response,  # Initial call with tools
            mock_followup_response,  # Follow-up with more tools
            mock_final_response,  # Final analysis
        ]

        print("üìä Running investigation...")
        print()

        # Run the actual investigation
        result = await investigator.investigate(test_query.strip())

        print("‚úÖ INVESTIGATION COMPLETE!")
        print("=" * 60)
        print()

        print("üìã RESULTS SUMMARY:")
        print("-" * 40)
        print(f"Original Query Length: {len(result['original_query'])} chars")
        print(f"Issue Type Detected: {result['parsed_query']['question_type']}")
        print(f"Investigation Length: {len(result['investigation'])} chars")
        print(f"Has Debug Info: {'debug' in result}")
        print()

        print("üîç INVESTIGATION FINDINGS:")
        print("-" * 40)
        print(result["investigation"])
        print()

        # Show the tool usage
        print("üõ†Ô∏è  TOOL USAGE ANALYSIS:")
        print("-" * 40)
        tool_calls = mock_anthropic.messages.create.call_args_list
        print(f"Total Anthropic API calls: {len(tool_calls)}")
        print(
            f"Tool executions: {investigator.tool_orchestrator.execute_tool.call_count}"
        )
        print("Tools used:")
        for call in investigator.tool_orchestrator.execute_tool.call_args_list:
            tool_name = call[0][0]  # First positional arg is tool name
            print(f"  - {tool_name}")
        print()

        print("üí° DYNAMIC BEHAVIOR DEMONSTRATED:")
        print("-" * 40)
        print("‚úÖ Detected access code issue type")
        print("‚úÖ Selected appropriate tools (device_info, access_codes, audit_logs)")
        print("‚úÖ Followed up with audit logs based on findings")
        print("‚úÖ Provided structured timeline analysis")
        print("‚úÖ Identified root cause (race condition)")
        print("‚úÖ Gave actionable recommendations")
        print()

        if "debug" in result:
            print("üîß DEBUG INFORMATION:")
            print("-" * 40)
            print(result["debug"]["log_summary"])

        return result


async def demo_limit_enforcement():
    """Demonstrate limit enforcement in action."""

    print("\n‚ö° DEMONSTRATING LIMIT ENFORCEMENT")
    print("=" * 60)

    # Use very restrictive limits to show enforcement
    restrictive_config = InvestigationConfig(
        MAX_TOOL_ROUNDS=1, MAX_TOOLS_PER_ROUND=2, MAX_TOTAL_TOOLS=2
    )

    print(
        f"Using restrictive limits: {restrictive_config.MAX_TOOL_ROUNDS} round, {restrictive_config.MAX_TOOLS_PER_ROUND} tools per round, {restrictive_config.MAX_TOTAL_TOOLS} total"
    )
    print()

    # This would show limit enforcement in a real scenario
    # For demo purposes, just show the logic
    from src.seam_agent.assistant.investigation_config import InvestigationState

    state = InvestigationState()
    print("üîÑ Simulating investigation with limits:")
    print()

    # Round 1
    if state.can_start_new_round(restrictive_config):
        state.start_new_round()
        print("‚úÖ Round 1: Started")

        # Tool 1
        if state.can_continue_round(restrictive_config):
            state.record_tool_use()
            print("  ‚úÖ Tool 1: get_device_info - SUCCESS")

        # Tool 2
        if state.can_continue_round(restrictive_config):
            state.record_tool_use()
            print("  ‚úÖ Tool 2: get_access_codes - SUCCESS")

        # Tool 3 (should be blocked)
        if state.can_continue_round(restrictive_config):
            state.record_tool_use()
            print("  ‚úÖ Tool 3: get_audit_logs - SUCCESS")
        else:
            print("  ‚ùå Tool 3: BLOCKED - Round limit reached (2 tools per round)")

    print()

    # Round 2 (should be blocked)
    if state.can_start_new_round(restrictive_config):
        print("‚úÖ Round 2: Started")
    else:
        print("‚ùå Round 2: BLOCKED - Max rounds (1) reached")

    print()
    print(f"Final state: {state.get_limits_summary()}")
    print()
    print("üí° Benefits:")
    print("  ‚úÖ Prevents runaway costs")
    print("  ‚úÖ Clear limit enforcement messages")
    print("  ‚úÖ Graceful handling without crashes")


async def main():
    """Run the full demo."""

    # Set required environment variables
    os.environ["OPENAI_API_KEY"] = "demo-key"
    os.environ["ANTHROPIC_API_KEY"] = "demo-key"
    os.environ["DATABASE_URL"] = "postgresql://demo:demo@localhost/demo"

    try:
        await demo_investigation_with_mocks()
        await demo_limit_enforcement()

        print("\nüéâ DEMO COMPLETE!")
        print("=" * 60)
        print("The enhanced SimpleInvestigator successfully demonstrated:")
        print("‚úÖ Dynamic tool selection based on issue type")
        print("‚úÖ Intelligent follow-up based on findings")
        print("‚úÖ Structured data processing and analysis")
        print("‚úÖ Limit enforcement and cost control")
        print("‚úÖ Clear, actionable investigation results")

    except Exception as e:
        print(f"Demo encountered an error: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
